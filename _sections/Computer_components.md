---
layout: default
title: "Computer components"
--- 

#### Processor

##### Basics of a CPU

Processors, or CPU's, are the "beating heart" of a computer, and as such, perform many more tasks than the ones that will be discussed in this text. The focus lies on the floating point calculation capabilities and the memory subsystems, as well as the interfaces.

The traditional CPU ([Central Processing Unit](https://en.wikipedia.org/wiki/Central_processing_unit)) is an Integrated Circuit (IC) that executes the logic, [arithmetic](https://en.wikipedia.org/wiki/Arithmetic), input/output (I/O) and control operations that are prescribed by software running on the computer. As time passed, many other subsystems of computers got integrated into the processor package (die), making the functions that the traditional CPU performs only a subset of all the functions that a modern processor performs. A "core" of a modern processor is a separate unit that performs all the tasks of a traditional CPU.

Modern processor cores are very diverse, complex and multi-faceted, varying wildly with ISA, microarchitecture, intended platform and manufacturer. A discussion about CPU's that would include all these variations would be impossible, necessitating a confinement. This discussion will try to be as generic as possible, but the focus lies on an Intel based server CPU of the "Skylake-SP" microarchitecture.

Intel dominates the PC/laptop as well as the HPC/Supercomputer CPU market, making the restriction towards an Intel x86-64 based CPU justified. Considering the fact that almost all Laptops and workstations contain CPU's that are (to a varying extend) derived from their server oriented counterparts, focusing the discussion around a server CPU seems logical as well. The Skylake-SP microarchitecture was chosen because it is very recent (at the time of writing), contains some very significant advancements for scientific computing workloads and is used in a cluster available to the MEFD group at the TU/e.

##### System on a Chip

Modern processors are best described by the ["System on a Chip" (SoC)](https://en.wikipedia.org/wiki/System_on_a_chip) moniker, containing many of the core components of a computer. As such, most contain the following subsystems:

 - Cores
 - Memory controller
 - Cache
 - Interfaces
 - Graphics (included in most consumer oriented CPU's)

##### Instruction set architecture

An Instruction Set Architecture ([ISA](https://en.wikipedia.org/wiki/Instruction_set_architecture)) is an abstract model of a computer and contains a collection of machine instruction definitions. Examples of common ISA's are x86-64, x86 and ARM, with x86-64 being the most common ISA for CPU's in servers, as well as consumer oriented computers.

An ISA is one of the most important aspects of a CPU, because it forms the link between software and hardware. ISA's where introduced to make programming software easier, which could now be written in terms of ISA instructions in stead of low level [machine code](https://en.wikipedia.org/wiki/Machine_code). This made it possible to execute the same computer program on different computers, without any modification of the code.

An implementation of an ISA, called a [microarchitecture (uarch)](https://en.wikipedia.org/wiki/Microarchitecture), is the hardware based realization of these machine instruction definitions (disregarding [microcode](https://en.wikipedia.org/wiki/Microcode)). Any specific uarch can also support extensions to its ISA, common examples are VT-d, AES-NI, SSE4 and AVX2. These extensions are additions to the abstract computer model of an ISA and contain specific instructions to accelerate certain tasks of a computer, like AES data encryption, virtualization and vector mathematics.

Some better known examples of microarchitectures are [Intel i386](https://en.wikipedia.org/wiki/Intel_80386), [Intel Nahelem](https://en.wikipedia.org/wiki/Nehalem_(microarchitecture)), [Intel Haswell](https://en.wikipedia.org/wiki/Haswell_%28microarchitecture%29), [AMD K8](https://en.wikipedia.org/wiki/AMD_K8), [AMD Bulldozer](https://en.wikipedia.org/wiki/Bulldozer_(microarchitecture)) and [AMD Zen](https://en.wikipedia.org/wiki/Zen_(microarchitecture)). 

##### Threads and cores

A [thread](https://en.wikipedia.org/wiki/Thread_(computing)) is a chain of instructions that is to be executed by the CPU. Each thread is generated by a process, which can loosely be described as an instance of a computer program. A single core of a CPU can execute one thread at a time, but using a technique called time slicing and the concept of [context switching](https://en.wikipedia.org/wiki/Context_switch), can handle multiple threads concurrently.

Multithreading (software) allows a single process to spawn a multitude of threads, dividing the workload of that process. Performance benefits (can) arise when these threads are executed in parallel on multiple cores of a CPU.

The specifications of a CPU may contain references to the number of threads it "has", which should be interpreted as the maximum amount of threads it can "execute" at the same time. The fact that a single CPU core can only execute one thread at a time doesn't change, even if the CPU specifications state that it has more (twice) threads then cores. This has to do with a hardware based technique called simultaneous multithreading ([SMT](https://en.wikipedia.org/wiki/Simultaneous_multithreading)), which will be discussed later.

##### Cache

Quick access to data is critical for the performance of a CPU, making data flow and storage a mayor aspect of a CPU. The main memory of a computer has a relatively high latency and low bandwidth compared to the needs of modern CPU cores, which is where [cache](https://en.wikipedia.org/wiki/CPU_cache) comes into play.

Cache is storage subsystem of the CPU and acts like a data buffer. It contains, among other things, copies of the data that the CPU (or process) "predicts" it will access often or in the near future, reducing the loading time of that data. The amount of cache placed on the CPU is relatively small, typically about 1:1.000, compared to the amount of main memory placed in a computer. A "cache-miss" refers to the situation where data is requested, but not stored in cache, resulting in a much longer loading time. Avoiding cache-misses is a large part of software (and hardware) optimization and can lead to very substantial performance improvements.

Cache memory pressure (memory nearly full) and the ever increasing speed of CPU cores lead to the development of multiple levels of cache. This layered structure has the advantage that it can address both the memory pressure problem as well as the demand for faster data access, without resulting in prohibitive costs. The upper most layer of cache, L1, has gotten significantly faster over time, but did not really increase much in capacity. The lowest level of cache, typically L3, saw the highest increase in capacity, but is also substantially slower then L1.

The development of multi-core processors and several levels of cache created an additional task for cache; inter-core data communication. Each core has its own private parts of L1 and L2 cache, whereas L3 cache is shared between the cores. This last level of cache is the place where data can be shared between the threads running on the different cores.

Cache memory is a lot faster, and generally superior on many fronts, compared to main memory, because cache is made from SRAM and main memory is made from DRAM. SRAM stands for "Static Random Access Memory", whereas DRAM stands for "Dynamic Random Access Memory". Each SRAM memory cell requires 6 transistors to store a bit, whereas DRAM requires only one transistor (and a small capacitor) per bit. The downside of DRAM is that the capacitors in DRAM memory need to be recharged frequently, causing delays and other problems. This constant refreshing of the stored data gave rise to the name "Dynamic", while "Static" was used for SRAM, because it doesn't need to be refreshed. The extra hardware complexity of SRAM allows it to be much faster than DRAM, but the extra cost and space requirements on the [die](https://en.wikipedia.org/wiki/Die_(integrated_circuit)) of the CPU also make it much more expensive.

##### Execution units and FMA3

Execution units of a CPU core are the parts that execute the machine instructions derived from the thread running on the core. There are many different types of execution units in modern CPU cores, each with their own specific function. Notable examples of execution units are; arithmetic logic unit ([ALU](https://en.wikipedia.org/wiki/Arithmetic_logic_unit)), address generation unit ([AGU](https://en.wikipedia.org/wiki/Address_generation_unit)) and floating-point unit ([FPU](https://en.wikipedia.org/wiki/Floating-point_unit)). Discussing the functions and operations of all these execution units is beyond the scope of this text, which will focus on the floating-point execution unit.

The floating-point execution units found in modern Intel CPU cores are FMA3 units (from Haswell on wards). These Fused Multiply Add units are capable of three different operations:

 - $ a = a \cdot c + b $
 - $ a = b \cdot a + c $
 - $ a = b \cdot c + a $

A single CPU core can contain multiple execution units of the same type, the exact amount varying with each microarchitecture. The Skylake-SP uarch contains 16 FMA3 units for "double" floating-point numbers and 32 FMA3 units for "single" floating-point numbers. These FMA3 units are separated into two groups called AVX-512 FMA units, which form the hardware layer of the [AVX-512](https://en.wikipedia.org/wiki/AVX-512) ISA extension.

##### SIMD

[SIMD](https://en.wikipedia.org/wiki/SIMD) stands for single instruction, multiple data and is a form of [data parallelism](https://en.wikipedia.org/wiki/Data_parallelism). SIMD is a vector processing technique, allowing the CPU core to execute the same instruction on multiple data entries (grouped in 1D arrays called vectors) in a single clock cycle. The maximum achievable throughput increases substantially by using SIMD, but does requires all the data to be manipulated in the same way, making it less versatile.

SIMD lies at the basis of almost all floating point execution units, like AVX2 and AVX-512. AVX2 execution units have 256 bit deep [registers](https://en.wikipedia.org/wiki/Processor_register) and AVX-512 execution units have 512 bit deep registers. These registers contain the data vectors, fitting 16/32 double/single floating-point numbers in case of AVX-512 and half that amount for AVX2.

##### Superscalar

As stated earlier, CPU cores can have many execution units. Keeping all the execution units busy at the same time requires multiple instructions to be dispatched (one instruction per execution unit) simultaneously. The ability of a CPU core to dispatch multiple instructions simultaneously is called being [superscalar](https://en.wikipedia.org/wiki/Superscalar_processor).

##### Simultaneous multithreading

Simultaneous multithreading (SMT), also known as [Hyper-Threading](https://en.wikipedia.org/wiki/Hyper-threading), is a technique aimed at increasing the utilization of a CPU core. Each physical CPU core represents multiple logical CPU cores, fully transparent to the Operating System ([OS](https://en.wikipedia.org/wiki/Operating_system)). Every logical CPU core gets their own thread assignment by the OS, meaning that a single CPU core is tasked with multiple threads. Switching between the assigned threads takes place when the running thread encounters a data request that requires multiple clock cycles to fulfill. The state of the current thread is saved after the data request and the state of the new thread gets loaded, allowing the CPU core to continue working on the new thread. This (should) prevent the execution units of the CPU core from being idle during data requests.

Simultaneous multithreading is quite similar to context switching, but allows for much faster switching, because the required logic is implemented in hardware.

The amount of logical cores per physical core differs from microarchitecture to microarchitecture. The most common is two logical cores per physical core, but Intel [Xeon Phi](https://en.wikipedia.org/wiki/Xeon_Phi) and IBM [POWER9](https://en.wikipedia.org/wiki/POWER9) based designs have 4 and up to 8 logical cores per physical core.

The performance improvements generated by SMT vary wildly with applications, from a factor of two down too a performance decrease. SMT has the largest positive effect in situations where cache-misses are frequent and the workloads of the threads are very heterogeneous. Math routines provided by highly optimized libraries, such as the Intel Math Kernel Library ([MKL](https://en.wikipedia.org/wiki/Math_Kernel_Library)), generally don't fall into this category, making SMT less viable for HPC purposes.

##### Operating frequency

The (high level) building blocks of digital circuits are called [logic gates](https://en.wikipedia.org/wiki/Logic_gate), which are hardware implementations of boolean operations. These logic gates are combined in intricate ways to provide more high level functionality, like the FMA operation on floating-point data. Synchronization of the logic gates is key for their operation, which is where the [clock signal](https://en.wikipedia.org/wiki/Clock_signal) comes into play. 

The clock signal of the CPU core is a square wave signal, switching between high and low (logical "on" and "off"). The rising and falling edges of this square wave are the "timing" signals for the logic gates to evaluate their input. The frequency of this timing signal is called the operating frequency or clock frequency of the CPU core. The operating frequency of a CPU core is directly linked to the throughput of [micro-operations](https://en.wikipedia.org/wiki/Micro-operation), making it a key factor in the performance of a CPU.

###### Turbo frequencies

Almost all modern CPU's employ some sort of dynamic operating frequency control, allowing the operating frequency of various parts of the CPU to go up or down in conjunction with demand and thermal headroom. Dynamic scaling of the operating frequencies took a flight when mobile devices became more popular, requiring momentary high performance and long battery life. The basic idea behind these techniques is that a relatively high operating frequency can be achieved for a short duration of time. This increases performance for workloads that can be completed within the time frame of the elevated operating frequency, but doesn't significantly increase the overall heat production and power consumption of the CPU. The turbo boost technology of modern CPU's is too complex to discuss in great detail in this text, but a few important aspects pertaining to floating point performance will be explained.

Specifications of the turbo frequencies are very important to the performance of a CPU, but are often reduced to a single number, masking the complete story for marketing purposes. Turbo frequencies scale down according to the workload type (normal, AVX2 and AVX512) and the amount of active cores. AVX512 workloads produce the most heat and highest power consumption because their execution units contain the most transistors, AVX2 execution units require less power and normal (non floating-point) workloads even less than that. 

A more detailed specification of the turbo frequencies of an Intel Xeon Gold 6132 CPU will be provided as an example. Intel ark based information on the [Xeon Gold 6132](https://ark.intel.com/products/123541/Intel-Xeon-Gold-6132-Processor-19_25M-Cache-2_60-GHz) specifies a base frequency of 2.6 GHz and a maximum turbo boost of 3.7 GHz. [WikiChip](https://en.wikichip.org/wiki/WikiChip) provides more details on [this](https://en.wikichip.org/wiki/intel/xeon_gold/6132#Frequencies) page. The highest floating-point performance of the Xeon Gold 6132 is achieved when all cores are utilizing their AVX512 execution units. The maximum turbo frequency of this workload is 2.3 GHz, which is about 40% lower than the maximum frequency (3.7 GHz) provided by Intel Ark.

#### CPU interfaces

As stated earlier, modern CPU's are more akin to SoC's than traditional processors. One of the most significant advancements has been the integration of interfaces onto the CPU package. This allows for higher bandwidth and lower latencies because fewer signals have to pass over [PCB](https://en.wikipedia.org/wiki/Printed_circuit_board) traces of the [motherboard](https://en.wikipedia.org/wiki/Motherboard).

##### Memory controller

The memory controller is the connecting element between the CPU cores and the DRAM memory. Many scientific computing workloads have data sets that don't fit in the cache of the CPU, forcing extensive usage of DRAM memory. These kinds of workloads usually benefit heavily from high bandwidth and low latency main memory, making the memory controller crucial for performance. Modern memory controllers are very complex pieces of engineering and explaining their operation is well beyond the scope of this text, which will focus on their features instead. 

One of the most distinguishing aspects of a memory controller is the amount of [memory channels](https://en.wikipedia.org/wiki/Multi-channel_memory_architecture) it supports.
A memory channel is a 64-bit wide interface to a cluster of DRAM chips, usually located on a [DIMM](https://en.wikipedia.org/wiki/DIMM). The peak bandwidth can be increased by allowing parallel access to multiple memory channels, making dual channel memory twice as fast as single channel memory, while latency remains unaffected. Typical consumer PC and laptop CPU's have an integrated memory controller capable of dual channel, whereas the Intel Skylake-SP chips contain two memory controller, each supporting triple channel for an effective 6 channel memory system.

A second important aspect of a memory controller/system is support for [Error Correcting Code (ECC)](https://en.wikipedia.org/wiki/ECC_memory), which is a technique to detect and correct certain memory errors. The information stored in a memory cell can get corrupted by a faulty power supply or interaction with solar radiation, resulting in a "bit flip". ECC capable memory stores additional bits of [parity data](https://en.wikipedia.org/wiki/RAM_parity) to detect and (when possible) repair these corruptions. Bit flips are not very common and most consumer applications don't suffer terribly when they encounter one (system may crash), but bit flips in sensitive, long running and expensive simulations are much more problematic. That is why ECC is almost always employed in servers, despite its drawbacks (higher cost and latency).

##### PCI-express

PCI-express ([Peripheral Component Interconnect Express](https://en.wikipedia.org/wiki/PCI_Express)) is the dominant interconnecting interface for computer components. Basically everything other then DRAM is connected to the CPU via (a derived form of) PCIe (PCI-express), making it a very important element of a computer. Some examples of components that are connected to the CPU via PCIe are:

  - [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit)
  - [NIC](https://en.wikipedia.org/wiki/Network_interface_controller)
  - [SAS controller](https://en.wikipedia.org/wiki/Serial_Attached_SCSI)
  - [NVMe storage](https://en.wikipedia.org/wiki/NVM_Express)

PCIe is a serial bus, introduced to replace [PCI(-X)](https://en.wikipedia.org/wiki/Conventional_PCI) and [AGP](https://en.wikipedia.org/wiki/Accelerated_Graphics_Port). The PCIe standard has seen multiple revisions (backwards compatible) over the years since its introduction, improving (among other things); bandwidth, power delivery, error correcting overhead and features. The most common implementation of the standard (at the time of writing) is version 3.x, which will be the version that this text considers.

PCIe links consist of "lanes", each lane having four physical connections. Two connections for a [differential](https://en.wikipedia.org/wiki/Differential_signaling) read signal and the other two for a differential write signal, amounting to a [full-duplex](https://en.wikipedia.org/wiki/Duplex_(telecommunications)#FULL-DUPLEX) connection. A PCIe link to a device may be a grouping of multiple lanes, ranging from one to 32 lanes per link (x1, x2, x4, x8, x16, x32). GPU's are commonly connected using a x16 link, SAS controllers and NVMe storage typically use a x4/x8 link and single port NIC's have a x1 link. 

The bandwidth of a PCIe v3 x1 link is specified using Giga Transfers per second (GT/s), which specifies the amount of bits that can be transferred from the host to the client or vice versa. The PCIe v3 standard uses 128b/130b encoding for error correcting purposes, meaning that for every 130 bits transmitted, only 128 bits contain data and the remaining two bits contain a form of parity data. This means that a PCIe v3 x1 link of 8 GT/s has a bandwidth of 985 MB/s (8000 x (128/130) x (1/8) = 984.62) and a x16 link has a bandwidth of 15.75 GB/s.

###### DMI

The Direct Media Interface (DMI) interconnect is an Intel specific protocol used to connect the CPU to the Platform Controller Hub ([PCH](https://en.wikipedia.org/wiki/Platform_Controller_Hub)), which is (among other things) responsible for USB and SATA connectivity. DMI is a prime example of an interconnect specification derived from PCIe, with a DMI 3.0 link being nearly equivalent to PCIe v3 x4 link.

##### QPI and UPI

Most high-end servers allow the placement of multiple identical CPU's on the same motherboard, allowing two or four CPU's to be part of the same computer. Intel QuickPath Interconnect ([QPI](https://en.wikipedia.org/wiki/Intel_QuickPath_Interconnect)) and its successor, Intel UltraPath Interconnect ([UPI](https://en.wikipedia.org/wiki/Intel_UltraPath_Interconnect)), are interfaces primarily used for inter CPU communication on these multi socket machines.

The bandwidth of the connection between the CPU's can be important in a number of scenarios, for example:

  - Memory bandwidth starved single threaded applications.
  - Threads running on CPU-0 that need to communicate with a GPU connected to CPU-1.
  - Results from threads running on CPU-0 and CPU-1 that need to be combined in a single thread.

The total bandwidth of a QPI/UPI connection is its transfer speed specification times four. The Intel Xeon Gold 6132 has a UPI link speed of 10.6 GT/s, amounting to 42.4 GB/s of bandwidth. Note that this is considerably less then the maximum memory bandwidth of the Xeon Gold 6132, which is 119.21 GiB/s.

#### Graphics card

##### NVIDIA Volta

There are many vendors of GPU's ([AMD](https://www.amd.com/en), [NVIDIA](https://www.nvidia.com/en-us/), [Intel](https://www.intel.com/content/www/us/en/homepage.html), [Imagination Technologies](https://www.imgtec.com/)), most of which have products capable of some sort of compute support via [OpenCL](https://www.khronos.org/opencl/). OpenCL stands for Open Compute Language and is a fully open source and portable framework for compute on CPU's and GPU's. The main competitor of OpenCL is [CUDA](https://en.wikipedia.org/wiki/CUDA), which is proprietary to NVIDIA.

CUDA is somewhat older than OpenCL and had a mature implementation before OpenCL, allowing CUDA to develop a head start with respect to the development of a GPGPU compute ecosystem. Combined with the very large dedicated graphics card market share of NVIDIA, results in CUDA being a dominant force in the GPGPU compute world. The most obvious disadvantage of this situation is vendor lock-in in NVIDIA's favor, which is reflected in this text, as it will focus on NVIDIA products.

Graphics cards are more divergent in their operation from microarchitecture to microarchitecture then CPU's are, making it more difficult to discuss certain topics in a general fashion. The main microarchitecture of interest to this text is [Volta](https://en.wikipedia.org/wiki/Volta_(microarchitecture)), as implemented by the Tesla V100 card. The Tesla V100 is a GPU (without display functionality) specifically developed for GPGPU purposes, featuring hardware capable of half and double precision floating
point operations, special "tensor" cores and very high memory bandwidth. These features are unavailable to most other NVIDIA GPU's (at the time of writing), making the Tesla V100 an obvious choice as GPGPU accelerator.

##### SIMD versus SIMT

##### Superscalar

##### Massively parallel